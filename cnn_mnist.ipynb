{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27059afa-cd42-4d43-82f4-19f040481606",
   "metadata": {},
   "source": [
    "# MNIST Digit Classification Model using CNN\n",
    "Reference: https://github.com/boaaaang/CNN-Implementation-in-Verilog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e2829e-af68-41f1-842f-f311031b36c2",
   "metadata": {},
   "source": [
    "## 1. Model Development\n",
    "Create and train the CNN model for MNIST digit classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34ab70bc-48c6-4cc8-b316-28ce546c27af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acd10971-6f7a-41fb-a2b7-d50e79c4ef03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2f9a501-3513-48b7-a777-4e27e17e5116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./data/',\n",
    "                               train=True,\n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "test_dataset = datasets.MNIST(root='./data/',\n",
    "                              train=False,\n",
    "                              transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2005f68c-c030-428e-ae66-9f04655a881b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader (Input Pipeline)\n",
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "057bd8e0-9b41-4a02-aafe-ab20211e7606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN class\n",
    "class CNN(nn.Module):\n",
    "     # Initialization\n",
    "    def __init__(self):\n",
    "        super (CNN, self).__init__()\n",
    "        \n",
    "        self.conv1_out_np = np.zeros((1, 3, 24, 24))\n",
    "        self.mp1_out_np = np.zeros((1, 3, 12, 12))\n",
    "        self.conv2_out_np = np.zeros((1, 3, 8, 8))\n",
    "        self.mp2_out_np = np.zeros((1, 3, 4, 4))\n",
    "        self.fc_in_np = np.zeros((1, 48))\n",
    "        self.fc_out_np = np.zeros((1, 10))\n",
    "        \n",
    "        # 1st Convolution Layer\n",
    "        # Image Input Shape -> (28, 28, 1)\n",
    "        # Convolution Layer -> (24, 24, 3)\n",
    "        # Pooling Max Layer -> (12, 12, 3)\n",
    "        self.conv1 = nn.Conv2d(1, 3, kernel_size=5)\n",
    "        \n",
    "        # 2nd Convolution Layer\n",
    "        # Image Input Shape -> (12, 12, 3)\n",
    "        # Convolution Layer -> (8, 8, 3)\n",
    "        # pooling Max Layer -> (4, 4, 3)\n",
    "        self.conv2 = nn.Conv2d(3, 3, kernel_size=5)\n",
    "        \n",
    "        # Max Pooling Layer\n",
    "        self.mp = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Fully Connected Layer\n",
    "        # Num of Weight = 480\n",
    "        self.fc_1 = nn.Linear(48, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        \n",
    "        # Layer Integration\n",
    "        x = self.conv1(x)\n",
    "        self.conv1_out_np = x.detach().numpy()\n",
    "        \n",
    "        x = F.relu(self.mp(x))\n",
    "        self.mp1_out_np = x.detach().numpy()\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        self.conv2_out_np = x.detach().numpy()\n",
    "        \n",
    "        x = F.relu(self.mp(x))\n",
    "        self.mp2_out_np = x.detach().numpy()\n",
    "        \n",
    "        # Flatten Layer\n",
    "        x = x.view(in_size, -1)\n",
    "        self.fc_in_np = x.detach().numpy()\n",
    "        \n",
    "        # Fully Connected Layer\n",
    "        x = self.fc_1(x)\n",
    "        self.fc_out_np = x.detach().numpy()\n",
    "        \n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a67147d-1df2-4181-900f-8e2c76c5322c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc_1): Linear(in_features=48, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiation\n",
    "model = CNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ce50abe-35d9-474b-b53e-2543463ad526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 796\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcce33a1-60a1-460e-b4d4-3d0fb2678d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1096cd8b-6fff-4529-8967-06d993925a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Ouput of feedforwarding\n",
    "        output = model(data)\n",
    "        \n",
    "        # Loss calibration\n",
    "        loss = F.nll_loss(output, target)\n",
    "        \n",
    "        # Gradient\n",
    "        loss.backward()\n",
    "        \n",
    "        # Back propagation\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "187cf7c1-54d6-4909-b6d7-7e94b34a1333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "def test():\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        \n",
    "        # Output of feedforwarding\n",
    "        output = model(data)\n",
    "        \n",
    "        test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "        \n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "          \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a19cda7f-2a31-4bcb-9354-1f08266bbda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.301875\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.283728\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.285463\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.282386\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.308233\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.258116\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.266397\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.266261\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.236049\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.246225\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.206519\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.186950\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.152501\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 2.141190\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 2.067386\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 1.970310\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.876815\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 1.743825\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 1.540556\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 1.393882\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.323723\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 1.050914\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 1.007735\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.820077\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.654861\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.533143\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.586571\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.445024\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.770903\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.649978\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.697907\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.537348\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.675109\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.723725\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.375689\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.627639\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.412068\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.498655\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.434976\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.448926\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.369940\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.335305\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.325853\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.444211\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.401974\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.425983\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.374857\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.255121\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.382060\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.392469\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.410984\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.249038\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.400917\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.399203\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.542241\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.329430\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.426452\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.520619\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.353961\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.327235\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.200829\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.391537\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.334481\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.220210\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.325988\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.194236\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.469635\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.262471\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.375605\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.252387\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.466667\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.342376\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.249881\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.187631\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.167903\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.206812\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.310399\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.205367\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.294381\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.256295\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.316543\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.492364\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.383606\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.216615\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.413058\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.264359\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.295817\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.205604\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.213401\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.325174\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.362033\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.313332\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.224158\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.312609\n",
      "\n",
      "Test set: Average loss: 0.2595, Accuracy: 9225/10000 (92%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.320257\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.288715\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.183621\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.206460\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.398525\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.138452\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.325737\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.191003\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.138890\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.165159\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.247490\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.191126\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.299898\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.230117\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.262451\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.359963\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.247233\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.281415\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.198792\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.208203\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.270255\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.272033\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.250609\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.162225\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.275536\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.323431\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.436714\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.224608\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.288087\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.346836\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.180602\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.285400\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.386409\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.322081\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.124459\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.338927\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.234627\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.315739\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.102249\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.211831\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.189681\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.233812\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.252122\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.177552\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.210419\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.221987\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.191501\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.473627\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.390598\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.329046\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.256615\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.225867\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.265186\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.422457\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.317977\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.347042\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.169860\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.287426\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.145676\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.113924\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.290303\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.152420\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.097041\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.146718\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.207068\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.240293\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.290006\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.183974\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.159550\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.384870\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.128263\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.225908\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.312001\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.135652\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.390226\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.214159\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.108822\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.176096\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.229618\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.147948\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.179265\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.361002\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.242618\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.260700\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.237751\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.099304\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.101098\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.240286\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.301662\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.120057\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.192538\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.228102\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.183557\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.275150\n",
      "\n",
      "Test set: Average loss: 0.2168, Accuracy: 9317/10000 (93%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.162645\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.130332\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.325113\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.237848\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.082427\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.227332\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.290776\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.154354\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.328968\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.166923\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.583914\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.057754\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.117617\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.328530\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.486745\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.300829\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.074885\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.260047\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.128371\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.269221\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.240271\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.132301\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.311898\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.206201\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.238730\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.271126\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.183045\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.147849\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.284072\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.094707\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.196708\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.211803\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.256982\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.232601\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.144041\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.298898\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.190747\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.102964\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.183042\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.155008\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.253437\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.175466\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.249758\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.122299\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.170798\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.154112\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.192285\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.285788\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.172199\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.112142\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.100649\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.246656\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.237865\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.184257\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.222515\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.144902\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.074331\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.335113\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.232368\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.142739\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.243401\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.147354\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.084435\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.290087\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.088193\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.132792\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.573486\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.211748\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.148314\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.187342\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.296906\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.393841\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.297297\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.205540\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.196155\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.196068\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.208902\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.295938\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.305060\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.273736\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.165985\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.076832\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.285644\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.222449\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.115785\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.155074\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.234033\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.229997\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.167439\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.086411\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.237637\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.266970\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.201158\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.287778\n",
      "\n",
      "Test set: Average loss: 0.1997, Accuracy: 9381/10000 (94%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.122633\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.264704\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.293599\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.266732\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.238658\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.209212\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.139190\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.191809\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.171064\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.215971\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.131143\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.047799\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.101091\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.166693\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.180578\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.148930\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.181607\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.064563\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.302496\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.227210\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.262849\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.405334\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.110564\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.153614\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.178767\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.267499\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.164924\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.393502\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.257518\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.049461\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.177702\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.120251\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.226952\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.053434\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.155802\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.079992\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.085954\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.094551\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.175015\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.050927\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.212591\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.176665\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.211403\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.335933\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.358037\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.171060\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.139988\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.113103\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.147373\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.308979\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.087200\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.288232\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.193687\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.173928\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.087123\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.113501\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.092018\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.308335\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.156176\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.207643\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.177018\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.148011\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.157437\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.067126\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.324740\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.257441\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.386259\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.105637\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.190605\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.062767\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.111549\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.058136\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.064452\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.122646\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.171821\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.171720\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.128467\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.231721\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.212289\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.247098\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.274333\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.226022\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.221617\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.254934\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.235729\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.274956\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.124839\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.192750\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.155779\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.066785\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.197201\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.129160\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.060840\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.165597\n",
      "\n",
      "Test set: Average loss: 0.1501, Accuracy: 9523/10000 (95%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.147218\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.099903\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.277969\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.169123\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.085562\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.175888\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.116212\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.286911\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.094866\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.159248\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.168837\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.167037\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.366185\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.250315\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.100268\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.206142\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.311440\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.144238\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.127660\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.096976\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.268249\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.271871\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.199728\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.294719\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.170138\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.141712\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.094123\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.187495\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.126448\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.127714\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.083922\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.167851\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.213102\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.215768\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.071945\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.141556\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.148151\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.182548\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.114266\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.119555\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.042479\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.237229\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.067400\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.180049\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.286871\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.100215\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.246225\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.207459\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.141115\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.209818\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.136449\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.068934\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.061651\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.142124\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.060735\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.070595\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.256409\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.171696\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.171833\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.307943\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.124681\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.152517\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.218666\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.019763\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.316906\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.056178\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.135344\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.181857\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.099734\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.076790\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.128065\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.195963\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.095323\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.249088\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.148812\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.233883\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.075521\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.173829\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.334514\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.189833\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.111646\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.119459\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.459022\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.267450\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.062843\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.187841\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.129864\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.214228\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.071476\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.097569\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.173833\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.199754\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.262412\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.228101\n",
      "\n",
      "Test set: Average loss: 0.1342, Accuracy: 9569/10000 (96%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.107791\n",
      "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.120235\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.184063\n",
      "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.168867\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.021742\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.145539\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.158022\n",
      "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.099748\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.181768\n",
      "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.271341\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.242895\n",
      "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.228162\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.109983\n",
      "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.097243\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.200489\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.133596\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.210091\n",
      "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.117188\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.135678\n",
      "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.244455\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.130042\n",
      "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.135527\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.131322\n",
      "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.235148\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.061192\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.156647\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.104990\n",
      "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.152087\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.175181\n",
      "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.127870\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.089585\n",
      "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.108239\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.041336\n",
      "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.166198\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.075463\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.119260\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.081835\n",
      "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.033851\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.204398\n",
      "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.052392\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.036686\n",
      "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.125131\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.277874\n",
      "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.193556\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.146608\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.095224\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.139974\n",
      "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.248491\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.148083\n",
      "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.051716\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.124482\n",
      "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.110141\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.210810\n",
      "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.127675\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.093904\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.041915\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.168662\n",
      "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.127365\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.132328\n",
      "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.224582\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.404809\n",
      "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.085231\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.116144\n",
      "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.354335\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.238236\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.210988\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.182708\n",
      "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.077020\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.162739\n",
      "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.117181\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.245574\n",
      "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.391515\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.041696\n",
      "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.169022\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.379307\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.109848\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.230517\n",
      "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.216341\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.163750\n",
      "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.155376\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.131116\n",
      "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.066124\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.136853\n",
      "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.193373\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.066249\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.201718\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.126811\n",
      "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.092581\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.261287\n",
      "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.202581\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.183789\n",
      "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.201861\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.135933\n",
      "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.191972\n",
      "\n",
      "Test set: Average loss: 0.1552, Accuracy: 9504/10000 (95%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.199603\n",
      "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.090342\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.190089\n",
      "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.075281\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.134755\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.136034\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.126114\n",
      "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.238000\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.174206\n",
      "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.085665\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.110989\n",
      "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.130376\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.192359\n",
      "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.061279\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.138190\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.134333\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.186312\n",
      "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.097978\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.075869\n",
      "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.250314\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.194894\n",
      "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.344739\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.416217\n",
      "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.137325\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.056086\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.212617\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.072316\n",
      "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.232262\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.068994\n",
      "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.214328\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.250323\n",
      "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.182112\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.056099\n",
      "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.176868\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.313242\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.192777\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.118343\n",
      "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.136492\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.249087\n",
      "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.025198\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.171131\n",
      "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.193425\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.035854\n",
      "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.096128\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.284358\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.071965\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.180926\n",
      "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.087282\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.119917\n",
      "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.107957\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.279623\n",
      "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.062812\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.049739\n",
      "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.174738\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.126179\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.034569\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.334046\n",
      "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.139968\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.525052\n",
      "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.111400\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.084326\n",
      "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.040745\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.083992\n",
      "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.185616\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.251650\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.199438\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.211960\n",
      "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.092801\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.049561\n",
      "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.150367\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.112093\n",
      "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.078877\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.322077\n",
      "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.127698\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.237725\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.064789\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.103884\n",
      "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.182425\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.132485\n",
      "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.104470\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.085017\n",
      "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.069106\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.301182\n",
      "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.138245\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.094482\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.094729\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.034707\n",
      "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.132421\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.197153\n",
      "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.048897\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.096275\n",
      "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.198064\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.119133\n",
      "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.106973\n",
      "\n",
      "Test set: Average loss: 0.1340, Accuracy: 9573/10000 (96%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.231273\n",
      "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.159514\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.167691\n",
      "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.105509\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.152564\n",
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.220883\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.060056\n",
      "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.157528\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.205340\n",
      "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.108098\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.134926\n",
      "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.110849\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.068574\n",
      "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.262186\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.234592\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.288642\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.049136\n",
      "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.087666\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.209753\n",
      "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.390161\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.075516\n",
      "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.195593\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.163728\n",
      "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.182452\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.150948\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.126053\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.147215\n",
      "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.118304\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.132807\n",
      "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.181494\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.260659\n",
      "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.246257\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.250454\n",
      "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.143285\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.107705\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.052633\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.095648\n",
      "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.172565\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.089712\n",
      "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.218563\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.119263\n",
      "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.148859\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.202834\n",
      "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.171455\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.087077\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.147393\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.318221\n",
      "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.126051\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.171719\n",
      "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.110446\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.228134\n",
      "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.148703\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.237058\n",
      "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.274811\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.284156\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.043001\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.083174\n",
      "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.055661\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.061565\n",
      "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.241140\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.076874\n",
      "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.148890\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.254377\n",
      "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.160858\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.192095\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.041356\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.168494\n",
      "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.182035\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.156286\n",
      "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.040498\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.228819\n",
      "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.084697\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.031606\n",
      "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.156933\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.048344\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.104027\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.165554\n",
      "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.077948\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.110050\n",
      "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.132611\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.097269\n",
      "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.239814\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.242490\n",
      "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.086208\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.154489\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.152033\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.427563\n",
      "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.068585\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.122345\n",
      "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.060413\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.076383\n",
      "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.058720\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.120621\n",
      "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.161824\n",
      "\n",
      "Test set: Average loss: 0.1238, Accuracy: 9601/10000 (96%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.150413\n",
      "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.081966\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.073097\n",
      "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.146453\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.250759\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.058368\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.172049\n",
      "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.056271\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.175695\n",
      "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.097703\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.035196\n",
      "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.078931\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.158207\n",
      "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.221723\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.229699\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.029880\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.179116\n",
      "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.094648\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.421793\n",
      "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.107194\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.503287\n",
      "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.095619\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.187513\n",
      "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.428871\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.072760\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.174388\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.117201\n",
      "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.020359\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.053705\n",
      "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.078076\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.144841\n",
      "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.083644\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.045499\n",
      "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.109464\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.134349\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.149011\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.073859\n",
      "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.180791\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.189620\n",
      "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.220744\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.191859\n",
      "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.307302\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.049113\n",
      "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.130054\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.193510\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.121132\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.291717\n",
      "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.178867\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.253205\n",
      "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.099883\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.110260\n",
      "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.120874\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.192280\n",
      "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.097685\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.096285\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.062953\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.168758\n",
      "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.226868\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.192832\n",
      "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.039003\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.055615\n",
      "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.086717\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.292595\n",
      "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.059549\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.221243\n",
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.189524\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.069270\n",
      "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.176082\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.214878\n",
      "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.127650\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.224981\n",
      "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.209782\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.189684\n",
      "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.020535\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.156767\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.066577\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.183689\n",
      "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.096231\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.191561\n",
      "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.095988\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.172623\n",
      "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.242480\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.133386\n",
      "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.196908\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.048958\n",
      "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.126591\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.112583\n",
      "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.238053\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.148028\n",
      "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.043070\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.106228\n",
      "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.141678\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.280562\n",
      "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.237222\n",
      "\n",
      "Test set: Average loss: 0.1139, Accuracy: 9635/10000 (96%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Traning process\n",
    "for epoch in range(1, 10):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79786e95-05b2-4bea-8e8e-486cdefd9822",
   "metadata": {},
   "source": [
    "## 2. Save Trained Weight and Bias\n",
    "Save trained weight and bias to a .pt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1cfeb96-50e8-4357-83e4-bf346349761d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model, \"./cnn_mnist.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "687d8033-10f7-4a7d-a8d0-783bc9a70eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc_1): Linear(in_features=48, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model = torch.load(\"./cnn_mnist.pt\", weights_only=False)\n",
    "model.eval()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a19b5ad-369f-4d83-b6e2-2a8ad20523bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 796\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a74bfe-24d6-4b7d-808b-3aeacb07f578",
   "metadata": {},
   "source": [
    "## 3. Testing using Bitmap Image\n",
    "Test the model to classify a digit from .bmp image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a88d3279-5916-4fb3-a736-a8d8a193671f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd38e884-ec86-46ed-bfa3-75fe51c84ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1e66f99fa10>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load bitmap image\n",
    "img = Image.open(\"./bmp/train_0.bmp\", \"r\")\n",
    "np_img = np.array(img)\n",
    "pyplot.imshow(np_img, cmap=pyplot.get_cmap('gray'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ada12b2e-313c-4601-9e17-27b289c1442e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted output: 5\n"
     ]
    }
   ],
   "source": [
    "np_img_re = np.reshape(np_img, (1,1,28,28))\n",
    "# 0 - 255 => 0 - 1 로 정규화, np.array => tensor 변환\n",
    "data = Variable(torch.tensor((np_img_re / 255), dtype = torch.float32))\n",
    "    \n",
    "# Output of feedforwarding\n",
    "output = model(data)\n",
    "pred = output.data.max(1, keepdim=True)[1]\n",
    "print('Predicted output: ' + ', '.join(map(str, pred.flatten().tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b897b6f7-4552-46fe-838f-dd23cffd98c0",
   "metadata": {},
   "source": [
    "## 4. Extract Weight and Bias\n",
    "Extract weight and bias to .mem files for RTL development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c4dbce6-7114-4768-a599-063ccdd09c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signed\n",
      "tensor([[ 28, -31, -27, -40,  -6],\n",
      "        [ 40,  72,  31,  59,  74],\n",
      "        [ 66,  68, 102,  95,  67],\n",
      "        [  4,  43,  42,  48,  45],\n",
      "        [ -2, -10, -57, -32,  -5]], dtype=torch.int32)\n",
      "tensor([[ 22, -16,  11,  42,  23],\n",
      "        [-10,  -2,  38,  63,  23],\n",
      "        [ 34,  45,  55,  56,  19],\n",
      "        [ 36,  10,  -8,  10,  29],\n",
      "        [ 31,  28,   0,  14,  52]], dtype=torch.int32)\n",
      "tensor([[-50, -27,  14,  47,  56],\n",
      "        [-69, -14,  20,  47,  20],\n",
      "        [-51, -38,  30,  66,  39],\n",
      "        [-36,   6,  32,  63,  33],\n",
      "        [-34,   4,  38,  58,  23]], dtype=torch.int32)\n",
      "tensor([ 10,  -1, -36], dtype=torch.int32)\n",
      "Unsigned\n",
      "tensor([[ 28, 225, 229, 216, 250],\n",
      "        [ 40,  72,  31,  59,  74],\n",
      "        [ 66,  68, 102,  95,  67],\n",
      "        [  4,  43,  42,  48,  45],\n",
      "        [254, 246, 199, 224, 251]], dtype=torch.int32)\n",
      "tensor([[ 22, 240,  11,  42,  23],\n",
      "        [246, 254,  38,  63,  23],\n",
      "        [ 34,  45,  55,  56,  19],\n",
      "        [ 36,  10, 248,  10,  29],\n",
      "        [ 31,  28,   0,  14,  52]], dtype=torch.int32)\n",
      "tensor([[206, 229,  14,  47,  56],\n",
      "        [187, 242,  20,  47,  20],\n",
      "        [205, 218,  30,  66,  39],\n",
      "        [220,   6,  32,  63,  33],\n",
      "        [222,   4,  38,  58,  23]], dtype=torch.int32)\n",
      "tensor([ 10, 255, 220], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "#################### Weight & Bias in HEX of Convolution Layer1 ####################\n",
    "\n",
    "# Calibration\n",
    "int_conv1_weight_1 = torch.tensor((model.conv1.weight.data[0][0] * 128), dtype = torch.int32)\n",
    "int_conv1_weight_2 = torch.tensor((model.conv1.weight.data[1][0] * 128), dtype = torch.int32)\n",
    "int_conv1_weight_3 = torch.tensor((model.conv1.weight.data[2][0] * 128), dtype = torch.int32)\n",
    "int_conv1_bias = torch.tensor((model.conv1.bias.data * 128), dtype = torch.int32)\n",
    "\n",
    "print(\"Signed\")\n",
    "print(int_conv1_weight_1)\n",
    "print(int_conv1_weight_2)\n",
    "print(int_conv1_weight_3)\n",
    "print(int_conv1_bias)\n",
    "\n",
    "# 2's Complement\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        if int_conv1_weight_1[i][j] < 0:\n",
    "            int_conv1_weight_1[i][j] += 256\n",
    "        if int_conv1_weight_2[i][j] < 0:\n",
    "            int_conv1_weight_2[i][j] += 256\n",
    "        if int_conv1_weight_3[i][j] < 0:\n",
    "            int_conv1_weight_3[i][j] += 256\n",
    "\n",
    "for k in range(3):\n",
    "    if int_conv1_bias[k] < 0:\n",
    "        int_conv1_bias[k] += 256\n",
    "\n",
    "print (\"Unsigned\")\n",
    "print(int_conv1_weight_1)\n",
    "print(int_conv1_weight_2)\n",
    "print(int_conv1_weight_3)\n",
    "print(int_conv1_bias)\n",
    "\n",
    "np.savetxt('conv1_weight_1.mem', int_conv1_weight_1, fmt='%1.2x',delimiter = \" \")\n",
    "np.savetxt('conv1_weight_2.mem', int_conv1_weight_2, fmt='%1.2x',delimiter = \" \")\n",
    "np.savetxt('conv1_weight_3.mem', int_conv1_weight_3, fmt='%1.2x',delimiter = \" \")\n",
    "np.savetxt('conv1_bias.mem', int_conv1_bias, fmt='%1.2x',delimiter = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46a308d0-6af3-4a54-9d04-765edb5c0e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signed\n",
      "tensor([[-17,  41,  36,  21,  22],\n",
      "        [ 11,  30,  25, -22,  -7],\n",
      "        [  6,   6, -36, -46, -11],\n",
      "        [  1, -29, -20,   1, -26],\n",
      "        [-56, -10,  -2,   6, -28]], dtype=torch.int32)\n",
      "tensor([[  0,  26,  36,  20,   7],\n",
      "        [  6,  18,  16,  -6,   1],\n",
      "        [ 28,   9, -19, -34,  -8],\n",
      "        [ -1, -27, -14,   3,  -5],\n",
      "        [-11, -11, -13, -12, -24]], dtype=torch.int32)\n",
      "tensor([[  9,  29,  17,  -9,  -1],\n",
      "        [ 34,  46,   5,  -7,  -6],\n",
      "        [ 48,  30, -24, -42, -11],\n",
      "        [ 43,   0, -16, -28, -25],\n",
      "        [ 16,   8,   1,  -3, -21]], dtype=torch.int32) \n",
      "\n",
      "tensor([[  0,  24,  44,  46,  57],\n",
      "        [ -2,  68,  99,  78,  51],\n",
      "        [-37, -67, -73, -44,   3],\n",
      "        [ 23,  11,   5, -15, -11],\n",
      "        [  3,  36,  16,   9,  -3]], dtype=torch.int32)\n",
      "tensor([[-10, -13,   7,   0,  40],\n",
      "        [-12,  20,   2,   3,  19],\n",
      "        [-36, -53, -49, -41,   6],\n",
      "        [ -9, -23, -11, -10,  11],\n",
      "        [ 15,  46,  43,  32,  -7]], dtype=torch.int32)\n",
      "tensor([[  0,  17, -24, -13, -29],\n",
      "        [-27, -46, -24, -53,  -3],\n",
      "        [-17, -55, -71, -40, -33],\n",
      "        [-22, -47, -49, -27, -19],\n",
      "        [ 27,  20,  22,  33,   0]], dtype=torch.int32) \n",
      "\n",
      "tensor([[-45,  -3,  22,  22, -18],\n",
      "        [ 17, -24, -27,  -4, -14],\n",
      "        [ 54,  14, -22, -17, -27],\n",
      "        [  1,  27,  17,   7,   3],\n",
      "        [  1,  46,  63,  61,  39]], dtype=torch.int32)\n",
      "tensor([[-37, -13,   1,  -3, -14],\n",
      "        [ 38,   7, -17,  -5, -19],\n",
      "        [ 43,   1, -29,  -8,  -8],\n",
      "        [ 19,   2,  13,   0, -23],\n",
      "        [ -3,  23,  28,  36,  12]], dtype=torch.int32)\n",
      "tensor([[  9,  -3,   8,  14,  -8],\n",
      "        [ 29, -11, -16,  -5,  -2],\n",
      "        [ 24, -15, -12, -30, -21],\n",
      "        [  0,  -9, -13, -22,  -3],\n",
      "        [ 14,   2,  -8,   0,   0]], dtype=torch.int32) \n",
      "\n",
      "tensor([13, 13,  9], dtype=torch.int32)\n",
      "Unsigned\n",
      "tensor([[239,  41,  36,  21,  22],\n",
      "        [ 11,  30,  25, 234, 249],\n",
      "        [  6,   6, 220, 210, 245],\n",
      "        [  1, 227, 236,   1, 230],\n",
      "        [200, 246, 254,   6, 228]], dtype=torch.int32)\n",
      "tensor([[  0,  26,  36,  20,   7],\n",
      "        [  6,  18,  16, 250,   1],\n",
      "        [ 28,   9, 237, 222, 248],\n",
      "        [255, 229, 242,   3, 251],\n",
      "        [245, 245, 243, 244, 232]], dtype=torch.int32)\n",
      "tensor([[  9,  29,  17, 247, 255],\n",
      "        [ 34,  46,   5, 249, 250],\n",
      "        [ 48,  30, 232, 214, 245],\n",
      "        [ 43,   0, 240, 228, 231],\n",
      "        [ 16,   8,   1, 253, 235]], dtype=torch.int32) \n",
      "\n",
      "tensor([[  0,  24,  44,  46,  57],\n",
      "        [254,  68,  99,  78,  51],\n",
      "        [219, 189, 183, 212,   3],\n",
      "        [ 23,  11,   5, 241, 245],\n",
      "        [  3,  36,  16,   9, 253]], dtype=torch.int32)\n",
      "tensor([[246, 243,   7,   0,  40],\n",
      "        [244,  20,   2,   3,  19],\n",
      "        [220, 203, 207, 215,   6],\n",
      "        [247, 233, 245, 246,  11],\n",
      "        [ 15,  46,  43,  32, 249]], dtype=torch.int32)\n",
      "tensor([[  0,  17, 232, 243, 227],\n",
      "        [229, 210, 232, 203, 253],\n",
      "        [239, 201, 185, 216, 223],\n",
      "        [234, 209, 207, 229, 237],\n",
      "        [ 27,  20,  22,  33,   0]], dtype=torch.int32) \n",
      "\n",
      "tensor([[211, 253,  22,  22, 238],\n",
      "        [ 17, 232, 229, 252, 242],\n",
      "        [ 54,  14, 234, 239, 229],\n",
      "        [  1,  27,  17,   7,   3],\n",
      "        [  1,  46,  63,  61,  39]], dtype=torch.int32)\n",
      "tensor([[219, 243,   1, 253, 242],\n",
      "        [ 38,   7, 239, 251, 237],\n",
      "        [ 43,   1, 227, 248, 248],\n",
      "        [ 19,   2,  13,   0, 233],\n",
      "        [253,  23,  28,  36,  12]], dtype=torch.int32)\n",
      "tensor([[  9, 253,   8,  14, 248],\n",
      "        [ 29, 245, 240, 251, 254],\n",
      "        [ 24, 241, 244, 226, 235],\n",
      "        [  0, 247, 243, 234, 253],\n",
      "        [ 14,   2, 248,   0,   0]], dtype=torch.int32) \n",
      "\n",
      "tensor([13, 13,  9], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "#################### Weight & Bias in HEX of Convolution Layer2 ####################\n",
    "\n",
    "# Calibration\n",
    "# print(np.shape(model.conv2.weight))\n",
    "int_conv2_weight_11 = torch.tensor((model.conv2.weight.data[0][0]* 128), dtype = torch.int32)\n",
    "int_conv2_weight_12 = torch.tensor((model.conv2.weight.data[0][1]* 128), dtype = torch.int32)\n",
    "int_conv2_weight_13 = torch.tensor((model.conv2.weight.data[0][2]* 128), dtype = torch.int32)\n",
    "\n",
    "int_conv2_weight_21 = torch.tensor((model.conv2.weight.data[1][0] * 128), dtype = torch.int32)\n",
    "int_conv2_weight_22 = torch.tensor((model.conv2.weight.data[1][1] * 128), dtype = torch.int32)\n",
    "int_conv2_weight_23 = torch.tensor((model.conv2.weight.data[1][2] * 128), dtype = torch.int32)\n",
    "\n",
    "int_conv2_weight_31 = torch.tensor((model.conv2.weight.data[2][0] * 128), dtype = torch.int32)\n",
    "int_conv2_weight_32 = torch.tensor((model.conv2.weight.data[2][1] * 128), dtype = torch.int32)\n",
    "int_conv2_weight_33 = torch.tensor((model.conv2.weight.data[2][2] * 128), dtype = torch.int32)\n",
    "\n",
    "int_conv2_bias = torch.tensor((model.conv2.bias.data * 128), dtype = torch.int32)\n",
    "\n",
    "print (\"Signed\")\n",
    "print(int_conv2_weight_11)\n",
    "print(int_conv2_weight_12)\n",
    "print(int_conv2_weight_13, '\\n')\n",
    "\n",
    "print(int_conv2_weight_21)\n",
    "print(int_conv2_weight_22)\n",
    "print(int_conv2_weight_23, '\\n')\n",
    "\n",
    "print(int_conv2_weight_31)\n",
    "print(int_conv2_weight_32)\n",
    "print(int_conv2_weight_33, '\\n')\n",
    "\n",
    "print(int_conv2_bias)\n",
    "\n",
    "# 2's Complement\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        if int_conv2_weight_11[i][j] < 0:\n",
    "            int_conv2_weight_11[i][j] += 256\n",
    "        if int_conv2_weight_12[i][j] < 0:\n",
    "            int_conv2_weight_12[i][j] += 256\n",
    "        if int_conv2_weight_13[i][j] < 0:\n",
    "            int_conv2_weight_13[i][j] += 256\n",
    "            \n",
    "        if int_conv2_weight_21[i][j] < 0:\n",
    "            int_conv2_weight_21[i][j] += 256\n",
    "        if int_conv2_weight_22[i][j] < 0:\n",
    "            int_conv2_weight_22[i][j] += 256\n",
    "        if int_conv2_weight_23[i][j] < 0:\n",
    "            int_conv2_weight_23[i][j] += 256\n",
    "            \n",
    "        if int_conv2_weight_31[i][j] < 0:\n",
    "            int_conv2_weight_31[i][j] += 256\n",
    "        if int_conv2_weight_32[i][j] < 0:\n",
    "            int_conv2_weight_32[i][j] += 256\n",
    "        if int_conv2_weight_33[i][j] < 0:\n",
    "            int_conv2_weight_33[i][j] += 256\n",
    "\n",
    "for k in range(3):\n",
    "    if int_conv2_bias[k] < 0:\n",
    "        int_conv2_bias[k] += 256\n",
    "\n",
    "print (\"Unsigned\")\n",
    "print(int_conv2_weight_11)\n",
    "print(int_conv2_weight_12)\n",
    "print(int_conv2_weight_13, '\\n')\n",
    "\n",
    "print(int_conv2_weight_21)\n",
    "print(int_conv2_weight_22)\n",
    "print(int_conv2_weight_23, '\\n')\n",
    "\n",
    "print(int_conv2_weight_31)\n",
    "print(int_conv2_weight_32)\n",
    "print(int_conv2_weight_33, '\\n')\n",
    "\n",
    "print(int_conv2_bias)\n",
    "\n",
    "np.savetxt('conv2_weight_11.mem', int_conv2_weight_11, fmt='%1.2x',delimiter = \" \")\n",
    "np.savetxt('conv2_weight_12.mem', int_conv2_weight_12, fmt='%1.2x',delimiter = \" \")\n",
    "np.savetxt('conv2_weight_13.mem', int_conv2_weight_13, fmt='%1.2x',delimiter = \" \")\n",
    "\n",
    "np.savetxt('conv2_weight_21.mem', int_conv2_weight_21, fmt='%1.2x',delimiter = \" \")\n",
    "np.savetxt('conv2_weight_22.mem', int_conv2_weight_22, fmt='%1.2x',delimiter = \" \")\n",
    "np.savetxt('conv2_weight_23.mem', int_conv2_weight_23, fmt='%1.2x',delimiter = \" \")\n",
    "\n",
    "np.savetxt('conv2_weight_31.mem', int_conv2_weight_31, fmt='%1.2x',delimiter = \" \")\n",
    "np.savetxt('conv2_weight_32.mem', int_conv2_weight_32, fmt='%1.2x',delimiter = \" \")\n",
    "np.savetxt('conv2_weight_33.mem', int_conv2_weight_33, fmt='%1.2x',delimiter = \" \")\n",
    "\n",
    "np.savetxt('conv2_bias.mem', int_conv2_bias, fmt='%1.2x',delimiter = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b47e73b3-65c0-40e6-b035-6ca33543b559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 48])\n",
      "tensor([[ -2,   4,   5, -15,  11,  12,  21,  -7,  15,  14, -45,   0,   6,   6,\n",
      "          -4,  29, -17,  14,   7,  -7, -43,   1,  32, -16, -46, -13,  -7, -12,\n",
      "          -5, -23,  -5,  17,  27,   4,   0,  42,   2, -21, -21, -13,   4,  41,\n",
      "          45, -45,   8,  57,  -8, -23],\n",
      "        [  7,   7,  51,  13,  21,   7,  34,   0,  15,  13,  33,  13,  16,  12,\n",
      "          30, -31,  32, -29, -24, -29,  -1, -15, -37, -46,  -4, -11, -27, -11,\n",
      "         -20,   1, -21,  10,   7,  19,   0, -35,  24, -39, -30,  30,  47, -41,\n",
      "          37,  35,  10,  -5,  20,  -6],\n",
      "        [  6, -15, -12, -12,  -3,   5,  -9, -33,   9,   3,  -4,  35,  49,  33,\n",
      "           8, -14,  35,  20,  17,  -4,  51,  22,  11, -41,   2,   3,  -8,   5,\n",
      "         -37,  -6, -28,  68,  10, -22, -29,  -8,  -6, -18, -17,   0, -18,  21,\n",
      "          51,  41,  14,  31,  -2,   9],\n",
      "        [-17, -47,  -7,  -7,  10,  -5, -21,  31, -23,   0,  13,  -6, -32, -24,\n",
      "         -23,   0,  45,  19,  18, -11,  59,   8,  18, -10,  21,   9,  41,   4,\n",
      "          21,  20,  10, -19, -18, -26, -12,   7, -18, -30,  27,   6,  22, -29,\n",
      "          26,   6,  22,   1,  -9,  -8],\n",
      "        [ -8,   8,   4,  25,  -9, -15, -14,  39,   6,  -4,  -8,  26,  -2, -11,\n",
      "          22,  11, -25, -58, -43, -44, -42, -40, -43, -44,  16,  20,   6,  28,\n",
      "          25,  -2,  22,   4, -21,   5, -22, -35,  34,  70,  14,   8,  -7,  15,\n",
      "         -47,  19, -23, -30, -16,  28],\n",
      "        [ 17,  23,  19,   4,   4,   8,  -5,  13, -13,   2, -18, -26, -14,  -4,\n",
      "           0, -28, -11,  -1,  17,  46, -18,  18,  13,  76,  37,  14,  20,   9,\n",
      "          19,  13,   0, -42, -21,  10,  21,  -4,  15, -20,   6, -13,  -5,   6,\n",
      "           4, -27,  -3,  26, -17,   5],\n",
      "        [  6,  36,  29,  43,  18,   0,  -2, -33,  10,  -2, -10, -36, -34,   8,\n",
      "          -4,  14, -29, -30,  -6,  52, -72, -60,  19,  29, -59, -22,  27,   7,\n",
      "         -17, -10,  -3,   6, -15,  21,  18,  13,  38,  28,   7,  -7,   4,  45,\n",
      "           6, -12, -36,  26,  -5, -21],\n",
      "        [-11, -24, -32,  13,  -5,  20,  -1,  35,  29,  46,  14,  18, -29, -10,\n",
      "          29,  42,  41,  31,   6,   6,  50,  26,  21,  -4,   3,  -9, -17,  17,\n",
      "          15, -11,   0,  23,  10,   6,   0,   1,  -4, -39, -29,   0, -26, -20,\n",
      "         -30,  10, -32, -39, -10,  -7],\n",
      "        [ -2,  -9, -23, -13,  -9, -38,  -2,  12,  -9,  -3,  11,  14,  14,  -7,\n",
      "         -18, -15, -26,   4,  39,   4,   7,  22,  12,   8,  20, -13, -16,  40,\n",
      "         -48,   0, -11,   2, -16, -10,   5,   8,  11,  38,  31,  -8, -55,   7,\n",
      "          -4,  18,  14,  36,  28, -12],\n",
      "        [-18,   5, -14, -48,  21,   7,  16,   7,   6, -25,  44,  37, -19, -18,\n",
      "          -9, -10, -37,   3,  16,  -3, -20,  27,  18,  -4,   6,  28,  22, -76,\n",
      "          48,  16, -15, -46,   0, -25, -16,  45, -10,  37,  -3,  21,  -1,   8,\n",
      "         -65, -19, -11, -15, -12,  25]], dtype=torch.int32)\n",
      "torch.Size([10])\n",
      "tensor([-12,  11, -17, -19,  18,   2,  16,   7,   7,   1], dtype=torch.int32)\n",
      "tensor([[254,   4,   5, 241,  11,  12,  21, 249,  15,  14, 211,   0,   6,   6,\n",
      "         252,  29, 239,  14,   7, 249, 213,   1,  32, 240, 210, 243, 249, 244,\n",
      "         251, 233, 251,  17,  27,   4,   0,  42,   2, 235, 235, 243,   4,  41,\n",
      "          45, 211,   8,  57, 248, 233],\n",
      "        [  7,   7,  51,  13,  21,   7,  34,   0,  15,  13,  33,  13,  16,  12,\n",
      "          30, 225,  32, 227, 232, 227, 255, 241, 219, 210, 252, 245, 229, 245,\n",
      "         236,   1, 235,  10,   7,  19,   0, 221,  24, 217, 226,  30,  47, 215,\n",
      "          37,  35,  10, 251,  20, 250],\n",
      "        [  6, 241, 244, 244, 253,   5, 247, 223,   9,   3, 252,  35,  49,  33,\n",
      "           8, 242,  35,  20,  17, 252,  51,  22,  11, 215,   2,   3, 248,   5,\n",
      "         219, 250, 228,  68,  10, 234, 227, 248, 250, 238, 239,   0, 238,  21,\n",
      "          51,  41,  14,  31, 254,   9],\n",
      "        [239, 209, 249, 249,  10, 251, 235,  31, 233,   0,  13, 250, 224, 232,\n",
      "         233,   0,  45,  19,  18, 245,  59,   8,  18, 246,  21,   9,  41,   4,\n",
      "          21,  20,  10, 237, 238, 230, 244,   7, 238, 226,  27,   6,  22, 227,\n",
      "          26,   6,  22,   1, 247, 248],\n",
      "        [248,   8,   4,  25, 247, 241, 242,  39,   6, 252, 248,  26, 254, 245,\n",
      "          22,  11, 231, 198, 213, 212, 214, 216, 213, 212,  16,  20,   6,  28,\n",
      "          25, 254,  22,   4, 235,   5, 234, 221,  34,  70,  14,   8, 249,  15,\n",
      "         209,  19, 233, 226, 240,  28],\n",
      "        [ 17,  23,  19,   4,   4,   8, 251,  13, 243,   2, 238, 230, 242, 252,\n",
      "           0, 228, 245, 255,  17,  46, 238,  18,  13,  76,  37,  14,  20,   9,\n",
      "          19,  13,   0, 214, 235,  10,  21, 252,  15, 236,   6, 243, 251,   6,\n",
      "           4, 229, 253,  26, 239,   5],\n",
      "        [  6,  36,  29,  43,  18,   0, 254, 223,  10, 254, 246, 220, 222,   8,\n",
      "         252,  14, 227, 226, 250,  52, 184, 196,  19,  29, 197, 234,  27,   7,\n",
      "         239, 246, 253,   6, 241,  21,  18,  13,  38,  28,   7, 249,   4,  45,\n",
      "           6, 244, 220,  26, 251, 235],\n",
      "        [245, 232, 224,  13, 251,  20, 255,  35,  29,  46,  14,  18, 227, 246,\n",
      "          29,  42,  41,  31,   6,   6,  50,  26,  21, 252,   3, 247, 239,  17,\n",
      "          15, 245,   0,  23,  10,   6,   0,   1, 252, 217, 227,   0, 230, 236,\n",
      "         226,  10, 224, 217, 246, 249],\n",
      "        [254, 247, 233, 243, 247, 218, 254,  12, 247, 253,  11,  14,  14, 249,\n",
      "         238, 241, 230,   4,  39,   4,   7,  22,  12,   8,  20, 243, 240,  40,\n",
      "         208,   0, 245,   2, 240, 246,   5,   8,  11,  38,  31, 248, 201,   7,\n",
      "         252,  18,  14,  36,  28, 244],\n",
      "        [238,   5, 242, 208,  21,   7,  16,   7,   6, 231,  44,  37, 237, 238,\n",
      "         247, 246, 219,   3,  16, 253, 236,  27,  18, 252,   6,  28,  22, 180,\n",
      "          48,  16, 241, 210,   0, 231, 240,  45, 246,  37, 253,  21, 255,   8,\n",
      "         191, 237, 245, 241, 244,  25]], dtype=torch.int32)\n",
      "tensor([244,  11, 239, 237,  18,   2,  16,   7,   7,   1], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "#################### Weight & Bias in HEX of Fully Connected Layer ####################\n",
    "\n",
    "print(np.shape(model.fc_1.weight))\n",
    "print((model.fc_1.weight * 128).int())\n",
    "\n",
    "print(np.shape(model.fc_1.bias))\n",
    "print((model.fc_1.bias * 128).int())\n",
    "\n",
    "int_fc_weight = (model.fc_1.weight * 128).int()\n",
    "int_fc_bias = (model.fc_1.bias * 128).int()\n",
    "\n",
    "# 2's Complement\n",
    "for i in range(10):\n",
    "    for j in range(48):\n",
    "        if int_fc_weight[i][j] < 0 :\n",
    "            int_fc_weight[i][j] += 256\n",
    "    if int_fc_bias[i] < 0 :\n",
    "        int_fc_bias[i] += 256\n",
    "        \n",
    "print(int_fc_weight)\n",
    "print(int_fc_bias)\n",
    "\n",
    "np.savetxt('fc_weight.mem', int_fc_weight, fmt='%1.2x',delimiter = \" \")\n",
    "np.savetxt('fc_bias.mem', int_fc_bias, fmt='%1.2x',delimiter = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d509ef6d-8096-4b9d-a978-f964fae2c571",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
